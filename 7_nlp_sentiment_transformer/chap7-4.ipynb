{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec, fastTextで日本語学習済みモデルを使用する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パッケージのインポート\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "#import torchvision\n",
    "#from torchvision import models,transforms\n",
    "import cv2 \n",
    "import time\n",
    "import math\n",
    "from matplotlib import cm\n",
    "\n",
    "import gensim\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecの日本語学習済みモデルを使用する実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#そのままではtorchtextで読み込めないので、gensimライブラリを使用してWord2vecのformatで保存し直します\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#一度gensimライブラリで読み込んでword2vecのformatで保存する\n",
    "model=KeyedVectors.load_word2vec_format(\n",
    "    \"./data/entity_vector/entity_vector.model.bin\",binary=True)\n",
    "\n",
    "#保存(10分くらい)\n",
    "#model.wv.save_word2vec_format(\"../7_nlp_sentiment_transformer/data/japanese_word2vec_vectors.vec\")\n",
    "model.save_word2vec_format(\"./data/japanese_word2vec_vectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchtextで単語ベクトルとして読み込みます\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_word2vec_vectors=Vectors(name=\"./data/japanese_word2vec_vectors.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1単語を表現する次元数: 200\n",
      "単語数： 1015474\n"
     ]
    }
   ],
   "source": [
    "#単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数:\",japanese_word2vec_vectors.dim)\n",
    "print(\"単語数：\",len(japanese_word2vec_vectors.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    #半角全角の統一\n",
    "    #今回は無視\n",
    "\n",
    "    #英語の小文字化\n",
    "    #今回は無視\n",
    "    #output=output.lower()\n",
    "\n",
    "    #改行、半角スペース、全角スペースを削除\n",
    "    text=re.sub(\"\\r\",\"\",text)\n",
    "    text=re.sub(\"\\n\",\"\",text)\n",
    "    text=re.sub(\"　\",\"\",text)\n",
    "    text=re.sub(\" \",\"\",text)\n",
    "\n",
    "    #数字文字の一律0化\n",
    "    text=re.sub(r\"[0-9 ０-９]\",\"0\",text) #数字\n",
    "    #記号と数字の除去\n",
    "    #今回は無視。半角記号・数字・英字\n",
    "    #今回は無視。全角記号\n",
    "\n",
    "    #特定の文字を正規表現で置換する\n",
    "    #今回は無視\n",
    "\n",
    "    return text\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "j_t=Tokenizer()\n",
    "\n",
    "def tokenizer_janome(text):\n",
    "    return [tok for tok in j_t.tokenize(text,wakati=True)]\n",
    "\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text=preprocessing_text(text) #前処理の正規化\n",
    "    ret=tokenizer_janome(text) #Janomeの単語分割\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "\n",
    "max_length=25\n",
    "\n",
    "TEXT=torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
    "use_vocab=True,lower=True,include_lengths=True, batch_first=True,fix_length=max_length)\n",
    "\n",
    "LABEL=data.Field(sequential=False,use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,val_ds,test_ds=torchtext.data.TabularDataset.splits(path=\"./data/\",\n",
    "train=\"text_train.tsv\",validation=\"text_val.tsv\",test=\"text_test.tsv\", format=\"tsv\",\n",
    "fields=[(\"Text\",TEXT),(\"Label\",LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 続き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds,vectors=japanese_word2vec_vectors,min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.6023, -2.6357, -2.5822,  ...,  0.6953, -1.4977,  1.4752],\n",
       "        ...,\n",
       "        [-1.5885,  0.1614, -0.6029,  ..., -1.7545, -1.2462,  2.3034],\n",
       "        [-0.3807,  0.4007, -1.6304,  ..., -2.2620, -0.5581,  0.9880],\n",
       "        [-3.2000,  2.9676, -0.9007,  ...,  1.8192, -0.4641,  0.3200]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ボキャブラリーのベクトルを確認\n",
    "print(TEXT.vocab.vectors.shape)\n",
    "TEXT.vocab.vectors\n",
    "\n",
    "#torch.Size([52, 200]) ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fcfac549690>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'と': 2,\n",
       "             '。': 3,\n",
       "             'な': 4,\n",
       "             'の': 5,\n",
       "             '文章': 6,\n",
       "             '、': 7,\n",
       "             'が': 8,\n",
       "             'し': 9,\n",
       "             'を': 10,\n",
       "             'いる': 11,\n",
       "             'か': 12,\n",
       "             'て': 13,\n",
       "             'ます': 14,\n",
       "             '分類': 15,\n",
       "             '本章': 16,\n",
       "             '評価': 17,\n",
       "             '0': 18,\n",
       "             'い': 19,\n",
       "             'から': 20,\n",
       "             'する': 21,\n",
       "             'その': 22,\n",
       "             'た': 23,\n",
       "             'で': 24,\n",
       "             'です': 25,\n",
       "             'に': 26,\n",
       "             'に対して': 27,\n",
       "             'は': 28,\n",
       "             'まし': 29,\n",
       "             'クラス': 30,\n",
       "             'ネガティブ': 31,\n",
       "             'ポジティブ': 32,\n",
       "             'モデル': 33,\n",
       "             'レビュー': 34,\n",
       "             '値': 35,\n",
       "             '処理': 36,\n",
       "             '取り組み': 37,\n",
       "             '商品': 38,\n",
       "             '女性': 39,\n",
       "             '女王': 40,\n",
       "             '好き': 41,\n",
       "             '姫': 42,\n",
       "             '学習': 43,\n",
       "             '構築': 44,\n",
       "             '機械': 45,\n",
       "             '王': 46,\n",
       "             '王子': 47,\n",
       "             '男性': 48,\n",
       "             '短い': 49,\n",
       "             '自然': 50,\n",
       "             '言語': 51})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女王： tensor(0.3840)\n",
      "王： tensor(0.3669)\n",
      "王子： tensor(0.5489)\n",
      "機械： tensor(-0.0867)\n"
     ]
    }
   ],
   "source": [
    "#姫-女性+男性のベクトルがどれと似ているか確認\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#姫-女性+男性\n",
    "tensor_calc=TEXT.vocab.vectors[42]-TEXT.vocab.vectors[39]+TEXT.vocab.vectors[48]\n",
    "\n",
    "#コサイン類似度を計算\n",
    "#dim=0は0次元目で計算してくださいという指定\n",
    "print(\"女王：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[40],dim=0))\n",
    "print(\"王：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[46],dim=0))\n",
    "print(\"王子：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[47],dim=0))\n",
    "print(\"機械：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[45],dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[39].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastTextの日本語学習済みモデルを使用する実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/351122 [00:00<?, ?it/s]Skipping token b'351122' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████| 351122/351122 [00:30<00:00, 11416.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1単語を表現する次元数： 300\n",
      "単語数： 351122\n"
     ]
    }
   ],
   "source": [
    "#torchtextで単語ベクトルとして読み込みます\n",
    "#word2vecとは異なり、すぐに読み込めます\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_fasttext_vectors=Vectors(name=\"../7_nlp_sentiment_transformer/data/vector_neologd/model.vec\")\n",
    "\n",
    "#単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", japanese_fasttext_vectors.dim)\n",
    "print(\"単語数：\",len(japanese_fasttext_vectors.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fcf90445550>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'と': 2,\n",
       "             '。': 3,\n",
       "             'な': 4,\n",
       "             'の': 5,\n",
       "             '文章': 6,\n",
       "             '、': 7,\n",
       "             'が': 8,\n",
       "             'し': 9,\n",
       "             'を': 10,\n",
       "             'いる': 11,\n",
       "             'か': 12,\n",
       "             'て': 13,\n",
       "             'ます': 14,\n",
       "             '分類': 15,\n",
       "             '本章': 16,\n",
       "             '評価': 17,\n",
       "             '0': 18,\n",
       "             'い': 19,\n",
       "             'から': 20,\n",
       "             'する': 21,\n",
       "             'その': 22,\n",
       "             'た': 23,\n",
       "             'で': 24,\n",
       "             'です': 25,\n",
       "             'に': 26,\n",
       "             'に対して': 27,\n",
       "             'は': 28,\n",
       "             'まし': 29,\n",
       "             'クラス': 30,\n",
       "             'ネガティブ': 31,\n",
       "             'ポジティブ': 32,\n",
       "             'モデル': 33,\n",
       "             'レビュー': 34,\n",
       "             '値': 35,\n",
       "             '処理': 36,\n",
       "             '取り組み': 37,\n",
       "             '商品': 38,\n",
       "             '女性': 39,\n",
       "             '女王': 40,\n",
       "             '好き': 41,\n",
       "             '姫': 42,\n",
       "             '学習': 43,\n",
       "             '構築': 44,\n",
       "             '機械': 45,\n",
       "             '王': 46,\n",
       "             '王子': 47,\n",
       "             '男性': 48,\n",
       "             '短い': 49,\n",
       "             '自然': 50,\n",
       "             '言語': 51})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ベクトル化したボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds,vectors=japanese_fasttext_vectors,min_freq=1)\n",
    "\n",
    "#ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape) #52個の単語が300次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n",
    "\n",
    "#ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女王： tensor(0.3650)\n",
      "王： tensor(0.3461)\n",
      "王子： tensor(0.5531)\n",
      "機械： tensor(0.1078)\n"
     ]
    }
   ],
   "source": [
    "#姫-女性+男性のベクトルがどれと似ているか確認\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#姫-女性+男性\n",
    "tensor_calc=TEXT.vocab.vectors[42]-TEXT.vocab.vectors[39]+TEXT.vocab.vectors[48]\n",
    "\n",
    "#コサイン類似度を計算\n",
    "#dim=0は0次元目で計算してくださいという指定\n",
    "print(\"女王：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[40],dim=0))\n",
    "print(\"王：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[46],dim=0))\n",
    "print(\"王子：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[47],dim=0))\n",
    "print(\"機械：\",F.cosine_similarity(tensor_calc,TEXT.vocab.vectors[45],dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eafc1c3d0ba1f317c23d0e9aa4360a9d8a5b5ceacbb507f9d0793fa2c9bc9e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
