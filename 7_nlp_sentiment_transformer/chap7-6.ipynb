{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの実装(分類タスク用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パッケージのインポート\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "#import torchvision\n",
    "#from torchvision import models,transforms\n",
    "import cv2 \n",
    "import time\n",
    "import math\n",
    "from matplotlib import cm\n",
    "\n",
    "import gensim\n",
    "import torchtext\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedderモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \"\"\" idで示されている単語をベクトルに変換します \"\"\" \n",
    "    def __init__(self,text_embedding_vectors):\n",
    "        super(Embedder,self).__init__()\n",
    "\n",
    "        self.embeddings=nn.Embedding.from_pretrained(embeddings=text_embedding_vectors, freeze=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_vec=self.embeddings(x)\n",
    "\n",
    "        return x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'posixpath' from '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python3.7/posixpath.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kawadatakeshiho/Desktop/pytorch_advanced/syakyo',\n",
       " '/Users/kawadatakeshiho/Desktop/pytorch_advanced/7_nlp_sentiment_transformer',\n",
       " '/Users/kawadatakeshiho/.vscode/extensions/ms-toolsai.jupyter-2022.9.1202862440/pythonFiles',\n",
       " '/Users/kawadatakeshiho/.vscode/extensions/ms-toolsai.jupyter-2022.9.1202862440/pythonFiles/lib/python',\n",
       " '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python37.zip',\n",
       " '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python3.7',\n",
       " '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python3.7/site-packages',\n",
       " '/Users/kawadatakeshiho/opt/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/kawadatakeshiho/.ipython']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前回分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "\n",
    "import re\n",
    "import random\n",
    "#import spacy\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "        # 改行コードを消去\n",
    "        text = re.sub('<br />', '', text)\n",
    "\n",
    "        # カンマ、ピリオド以外の記号をスペースに置換\n",
    "        for p in string.punctuation:\n",
    "            if (p == \".\") or (p == \",\"):\n",
    "                continue\n",
    "            else:\n",
    "                text = text.replace(p, \" \")\n",
    "\n",
    "        # ピリオドなどの前後にはスペースを入れておく\n",
    "        text = text.replace(\".\", \" . \")\n",
    "        text = text.replace(\",\", \" , \")\n",
    "        return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "        text = preprocessing_text(text)\n",
    "        ret = tokenizer_punctuation(text)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMDb_DataLoaders_and_TEXT(max_length=256, batch_size=24):\n",
    "    \"\"\"IMDbのDataLoaderとTEXTオブジェクトを取得する。 \"\"\"\n",
    "    # データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "    # max_length\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "    # フォルダ「data」から各tsvファイルを読み込みます\n",
    "    train_val_ds, test_ds = torchtext.data.TabularDataset.splits(path='./data/', train='IMDb_train.tsv',test='IMDb_test.tsv', format='tsv',fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "    # torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "    train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "    # torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
    "    english_fasttext_vectors = Vectors(name='./data/wiki-news-300d-1M.vec')\n",
    "\n",
    "    # ベクトル化したバージョンのボキャブラリーを作成します\n",
    "    TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "    # DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "    train_dl = torchtext.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "    val_dl = torchtext.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    return train_dl, val_dl, test_dl, TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　前回分終わり\n",
    "###  動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#動作確認\n",
    "#前節のDataLoaderなどを取得\n",
    "#from utils.fixing_dataloader import get_IMDb_DataLoaders_and_TEXT\n",
    "train_dl,val_dl,test_dl,TEXT=get_IMDb_DataLoaders_and_TEXT(max_length=256,batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([24, 256])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "#ミニバッチの用意\n",
    "batch=next(iter(train_dl))\n",
    "\n",
    "#モデル構築\n",
    "net1=Embedder(TEXT.vocab.vectors)\n",
    "\n",
    "#入出力\n",
    "x=batch.Text[0]\n",
    "x1=net1(x) #単語をベクトルに\n",
    "\n",
    "print(\"入力のテンソルサイズ：\",x.shape)\n",
    "print(\"出力のテンソルサイズ：\",x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3116,  0.0856, -0.0069,  ...,  0.0877,  0.1019,  0.0097],\n",
       "         [-0.0303,  0.0524, -0.0671,  ...,  0.2191, -0.1200,  0.0505],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0047,  0.0223, -0.0087,  ...,  0.1479,  0.1324, -0.0318],\n",
       "         [-0.1289,  0.0115,  0.0033,  ...,  0.1811,  0.0398, -0.0530],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0047,  0.0223, -0.0087,  ...,  0.1479,  0.1324, -0.0318],\n",
       "         [-0.1289,  0.0115,  0.0033,  ...,  0.1811,  0.0398, -0.0530],\n",
       "         ...,\n",
       "         [ 0.0037, -0.1019,  0.0102,  ...,  0.1508, -0.0391, -0.2279],\n",
       "         [ 0.0242, -0.0265,  0.0822,  ...,  0.0831, -0.0466,  0.0315],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0276,  0.0867,  0.0370,  ..., -0.0237,  0.0673,  0.0294],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.1152, -0.0489,  0.1490,  ...,  0.1617,  0.0246,  0.0154],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0622,  0.0350,  0.0789,  ...,  0.1830,  0.0532,  0.0857],\n",
       "         [ 0.0897,  0.0160, -0.0571,  ...,  0.1559, -0.0254, -0.0259],\n",
       "         ...,\n",
       "         [ 0.0047,  0.0223, -0.0087,  ...,  0.1479,  0.1324, -0.0318],\n",
       "         [-0.1114, -0.0220,  0.0287,  ...,  0.0893,  0.0886, -0.0707],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0400, -0.0524,  0.0961,  ...,  0.2066, -0.2227,  0.0333],\n",
       "         [ 0.0156,  0.0752, -0.0780,  ...,  0.0882, -0.0882, -0.0096],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalEncoderモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\" 入力された単語の位置を示すベクトル情報を付加する \"\"\"\n",
    "\n",
    "    def __init__(self,d_model=300,max_seq_len=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model=d_model #単語ベクトルの次元数\n",
    "\n",
    "        #単語の順番(pos)と埋め込みベクトルの次元の位置(i)によって一意に定まる値の表をpeとして作成\n",
    "        pe=torch.zeros(max_seq_len,d_model)\n",
    "\n",
    "        #GPU使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i]=math.sin(pos/(10000**((2*i)/d_model)))\n",
    "                pe[pos,i+1]=math.cos(pos/(10000**((2*(i+1))/d_model)))\n",
    "\n",
    "        #表peの先頭に、ミニバッチ次元となる次元をたす\n",
    "        self.pe=pe.unsqueeze(0)\n",
    "\n",
    "        #勾配を計算しないようにする\n",
    "        self.pe.requires_grad=False\n",
    "\n",
    "    def forward(self,x):\n",
    "        #入力xとPositional Encodingを足し算する\n",
    "        #xがpeよりも小さいので、大きくする\n",
    "        ret=math.sqrt(self.d_model)*x+self.pe\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "\n",
    "#モデル構築\n",
    "net1=Embedder(TEXT.vocab.vectors)\n",
    "net2=PositionalEncoder(d_model=300,max_seq_len=256)\n",
    "\n",
    "#入出力\n",
    "x=batch.Text[0]\n",
    "x1=net1(x) #単語をベクトルに\n",
    "x2=net2(x1)\n",
    "\n",
    "print(\"入力のテンソルサイズ：\",x1.shape)\n",
    "print(\"出力のテンソルサイズ：\",x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlockモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Transformerは本当はマルチヘッドAttentionですが、わかりやすさを優先しシングルAttentionで実装 \"\"\"\n",
    "\n",
    "    def __init__(self,d_model=300):\n",
    "        super().__init__()\n",
    "\n",
    "        #SAGANではadConvを使用したが、今回は全結合層で特徴量を変換する\n",
    "        self.q_linear=nn.Linear(d_model,d_model)\n",
    "        self.v_linear=nn.Linear(d_model,d_model)\n",
    "        self.k_linear=nn.Linear(d_model,d_model)\n",
    "\n",
    "        #出力時に使用する全結合層\n",
    "        self.out=nn.Linear(d_model,d_model)\n",
    "\n",
    "        #Attentionの大きさ調節の変数\n",
    "        self.d_k=d_model\n",
    "\n",
    "    def forward(self,q,k,v,mask):\n",
    "        #全結合層で特徴量を変換\n",
    "        k=self.k_linear(k)\n",
    "        q=self.q_linear(q)\n",
    "        v=self.v_linear(v)\n",
    "\n",
    "        #Attentionの値を計算する\n",
    "        #各値を足し算すると大きくなりすぎるので、root(d_k)でわって調整\n",
    "        weights=torch.matmul(q,k.transpose(1,2))/math.sqrt(self.d_k)\n",
    "\n",
    "        #ここでmaskを計算\n",
    "        mask=mask.unsqueeze(1)\n",
    "        weights=weights.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        #softmaxで規格化をする\n",
    "        normalized_weights=F.softmax(weights,dim=-1)\n",
    "\n",
    "        #AttentionとValueを掛け算\n",
    "        output=torch.matmul(normalized_weights,v)\n",
    "\n",
    "        #全結合層で特徴量を変換\n",
    "        output=self.out(output)\n",
    "\n",
    "        return output,normalized_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff=1024,dropout=0.1):\n",
    "        \"\"\" Attention層から出力を単純に全結合層2つで特徴量に変換するだけのユニットです \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1=nn.Linear(d_model,d_ff)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear_2=nn.Linear(d_ff,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.linear_1(x)\n",
    "        x=self.dropout(F.relu(x))\n",
    "        x=self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #LayerNormalization層\n",
    "        self.norm_1=nn.LayerNorm(d_model)\n",
    "        self.norm_2=nn.LayerNorm(d_model)\n",
    "\n",
    "        #Attention層\n",
    "        self.attn=Attention(d_model)\n",
    "\n",
    "        #Attentionの後の全結合層2つ\n",
    "        self.ff=FeedForward(d_model)\n",
    "\n",
    "        #Dropout\n",
    "        self.dropout_1=nn.Dropout(dropout)\n",
    "        self.dropout_2=nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        #正規化とAttention\n",
    "        x_normalized=self.norm_1(x)\n",
    "        output,normalized_weights=self.attn(x_normalized,x_normalized,x_normalized,mask)\n",
    "\n",
    "        x2=x+self.dropout_1(output)\n",
    "\n",
    "        #正規化と全結合層\n",
    "        x_normalized2=self.norm_2(x2)\n",
    "        output=x2+self.dropout_2(self.ff(x_normalized2))\n",
    "\n",
    "        return output,normalized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])\n",
      "入力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "出力のテンソルサイズ： torch.Size([24, 256, 300])\n",
      "Attentionのサイズ: torch.Size([24, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "\n",
    "#モデル確認\n",
    "net1=Embedder(TEXT.vocab.vectors)\n",
    "net2=PositionalEncoder(d_model=300,max_seq_len=256)\n",
    "net3=TransformerBlock(d_model=300)\n",
    "\n",
    "#maskの作成\n",
    "x=batch.Text[0]\n",
    "input_pad=1 #単語IDにおいて<pad>:1のため\n",
    "input_mask=(x!=input_pad)\n",
    "print(input_mask[0])\n",
    "\n",
    "#入出力\n",
    "x1=net1(x) #単語をベクトルに\n",
    "x2=net2(x1) #Position情報を足し算\n",
    "x3,normlizes_weights=net3(x2,input_mask) #Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\",x2.shape)\n",
    "print(\"出力のテンソルサイズ：\",x3.shape)\n",
    "print(\"Attentionのサイズ:\",normlizes_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassificationHeadモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\" Transformer_Blockの出力を使用し、最後のクラス分類をさせる \"\"\"\n",
    "    def __init__(self,d_model=300,output_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        #全結合層\n",
    "        self.linear=nn.Linear(d_model,output_dim) #output_dimはポジ・ネガの2つ\n",
    "\n",
    "        #重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight,std=0.02)\n",
    "        nn.init.normal_(self.linear.bias,0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0=x[:,0,:] #各ミニバッチの各文の先頭の単語の特徴量(300次元)を取り出す\n",
    "        out=self.linear(x0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最終的なTransformerモデルのクラス\n",
    "\n",
    "class TransformerClassification(nn.Module):\n",
    "    \"\"\" Transformerでクラス分類させる \"\"\"\n",
    "\n",
    "    def __init__(self,text_embedding_vectors,d_model=300,max_seq_len=256,output_dim=2):\n",
    "        super().__init__()\n",
    "\n",
    "        #モデル構築\n",
    "        self.net1=Embedder(text_embedding_vectors)\n",
    "        self.net2=PositionalEncoder(d_model=d_model,max_seq_len=max_seq_len)\n",
    "        self.net3_1=TransformerBlock(d_model=d_model)\n",
    "        self.net3_2=TransformerBlock(d_model=d_model)\n",
    "        self.net4=ClassificationHead(output_dim=output_dim,d_model=d_model)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        x1=self.net1(x) #単語をベクトルに\n",
    "        x2=self.net2(x1) #Position情報を足し算\n",
    "        x3_1,normlizes_weights_1=self.net3_1(x2,mask) #Self-Attentionで特徴量を変換\n",
    "        x3_2,normalizes_weights_2=self.net3_2(x3_1,mask) #Self-Attentionで特徴量を変換\n",
    "        x4=self.net4(x3_2)\n",
    "\n",
    "        return x4,normlizes_weights_1,normalizes_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力のテンソルサイズ： torch.Size([24, 2])\n",
      "出力テンソルのsigmoid： tensor([[0.2452, 0.7548],\n",
      "        [0.2741, 0.7259],\n",
      "        [0.2847, 0.7153],\n",
      "        [0.2676, 0.7324],\n",
      "        [0.2755, 0.7245],\n",
      "        [0.2989, 0.7011],\n",
      "        [0.2412, 0.7588],\n",
      "        [0.2641, 0.7359],\n",
      "        [0.2555, 0.7445],\n",
      "        [0.2290, 0.7710],\n",
      "        [0.2819, 0.7181],\n",
      "        [0.2626, 0.7374],\n",
      "        [0.2771, 0.7229],\n",
      "        [0.2367, 0.7633],\n",
      "        [0.2540, 0.7460],\n",
      "        [0.2520, 0.7480],\n",
      "        [0.2669, 0.7331],\n",
      "        [0.2418, 0.7582],\n",
      "        [0.3119, 0.6881],\n",
      "        [0.2459, 0.7541],\n",
      "        [0.2456, 0.7544],\n",
      "        [0.2988, 0.7012],\n",
      "        [0.2252, 0.7748],\n",
      "        [0.2257, 0.7743]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "#ミニバッチの用意\n",
    "batch=next(iter(train_dl))\n",
    "\n",
    "#モデル構築\n",
    "net=TransformerClassification(\n",
    "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
    "\n",
    "#入出力\n",
    "x=batch.Text[0]\n",
    "input_mask=(x!=input_pad)\n",
    "out,normlizes_weights_1,normlizes_weights_2=net(x,input_mask)\n",
    "\n",
    "print(\"出力のテンソルサイズ：\",out.shape)\n",
    "print(\"出力テンソルのsigmoid：\",F.softmax(out,dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8eafc1c3d0ba1f317c23d0e9aa4360a9d8a5b5ceacbb507f9d0793fa2c9bc9e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
